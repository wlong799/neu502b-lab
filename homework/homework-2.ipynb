{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU502B: Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework assignment, you will work through three commonly used methods in cognitive computational neuroscience: (1) neural decoding via multivariate pattern analysis (MVPA); (2) representational similarity analysis (RSA); and (3) voxelwise encoding analysis using regularized regression. Each of these problems builds on tools and ideas we've introduced in the in-class lab notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Multivariate pattern classification\n",
    "\n",
    "First, we'll start with a simple example of classifying distributed response patterns for different object categories from [Haxby et al., 2001](https://doi.org/10.1126/science.1063736). We'll begin by loading in the data, as well as labels for the stimuli and runs. You'll need to change `data_dir` to a directory on your computer (or the server); if you've already downloaded this dataset in lab, you can set `data_dir` to the existing directory to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn.image import index_img\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Haxby et al., 2001 data via Nilearn\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=\"data\")\n",
    "\n",
    "# Load in session metadata as pandas DataFrame\n",
    "session = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "\n",
    "# Extract stimuli and run labels for this subject\n",
    "stimuli, runs = session[\"labels\"].values, session[\"chunks\"].values\n",
    "\n",
    "# Create a boolean array indexing TRs containing a stimulus (non-rest)\n",
    "task_trs = stimuli != \"rest\"\n",
    "\n",
    "# Get list of unique stimulus categories (excluding rest)\n",
    "categories = [c for c in np.unique(stimuli) if c != \"rest\"]\n",
    "\n",
    "# Extract task TRs for fMRI data and stimulus/run labels\n",
    "func_task = index_img(haxby_dataset.func[0], task_trs)\n",
    "stimuli_task = stimuli[task_trs]\n",
    "runs_task = runs[task_trs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `NiftiMasker` (with `standardize=True`) to create a masker for ventral temporal (VT) cortex. Use the masker to extract the the NumPy array containing the functional data. (We'll analyze the data using scikit-learn rather than nilearn.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the VT mask file and creater masker:\n",
    "from nilearn.maskers import NiftiMasker\n",
    "\n",
    "\n",
    "# Uses masker to extract numpy array for VT:\n",
    "masker = NiftiMasker(mask_img=haxby_dataset.mask_vt[0], standardize=True)\n",
    "func_vt = masker.fit_transform(func_task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll set up a full SVM classification analysis using leave-one-run-out outer cross-validation with a nested leave-one-run-out inner cross-validation loop for grid search across the values of the SVM regularization parameter $C$. Sounds like a lot! But scikit-learn makes it pretty straightforward. First, initialize the `LinearSVC` estimator. Since this well-behaved dataset has the same number of samples for each stimulus category in each run, we can perform leave-one-run-out cross-validation using just `KFold` rather than having to specify the runs directly. Initalize an outer `KFold` cross-validator with 12 splits and an inner `KFold` cross-validator with 11 splits. We'll search over a handful of $C$ parameters: `param_grid = {'C': [1e-2, 1e-1, 1]}`. Initialize the `GridSearchCV` estimator with the SVM estimator, the parameter grid, and the inner cross-validator; then, submit this estimator to `cross_val_predict` with the outer cross-validator to run the full analysis. (This may take a few minutes to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV, KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Suppress some warnings (e.g. SVM convergence) just to clean up output\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# Initialize SVM and outer/inner CVs:\n",
    "num_runs = len(np.unique(runs_task))\n",
    "svm = LinearSVC()\n",
    "outer_cv = KFold(n_splits=num_runs)\n",
    "inner_cv = KFold(n_splits=num_runs - 1)\n",
    "\n",
    "# Set up parameter grid:\n",
    "param_grid = {\"C\": [1e-2, 1e-1, 1]}\n",
    "\n",
    "# Initialize GridSearchCV estimator:\n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=inner_cv)\n",
    "\n",
    "# Generate predictions using cross_val_predict:\n",
    "predictions = cross_val_predict(\n",
    "    svm_cv, func_vt, stimuli_task, cv=outer_cv, n_jobs=-1, verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the resulting predictions. We'll evaluate our classifier's predictions in two ways. First, use `accuracy_score` from `sklearn.metrics` to evaluate the predictions (across all test sets) against the actual labels in terms of a single classification accuracy. Procedurally, this is slightly different from computing accuracies on the test for each fold and averaging themâ€”but the resulting value should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy score:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(stimuli_task, predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand what our classifer is doing (i.e. what it's getting right and what it's getting wrong), we'll construct a confusion matrix. Construct the confusion matrix from the actual stimulus labels and the classifer's predicted labels and plot it below. What categories does the classifier tend to misclassify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix from true and predicted labels:\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "conf_mat = confusion_matrix(stimuli_task, predictions)\n",
    "\n",
    "\n",
    "# Plot confusion matrix:\n",
    "disp = ConfusionMatrixDisplay(conf_mat, display_labels=categories)\n",
    "disp.plot(xticks_rotation=\"vertical\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The classifier tends to mix up the small inanimate objects (scissor, bottle, chair, and shoe),\n",
    "> performing particularly poorly on scissors. It also occassionally misclassifies cats as chair\n",
    "> (four legs?) and face (animacy?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll repeat the same analysis for functional regions of interest (ROIs) maximally responsive to faces (roughly FFA) and houses (roughly PPA). Use the `mask_face` and `mask_house` files from the dataset to create an FFA masker and a PPA masker; extract the functional data for both. Submit these datasets to the same analysis as above, and visualize the results in terms of an overall accuracy score and confusion matrix. Interpret the accuracies and confusion matrices in light of the expected chance accuracy, given what you know about these ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masker for FFA:\n",
    "ffa_masker = NiftiMasker(mask_img=haxby_dataset.mask_face[0], standardize=True)\n",
    "\n",
    "# Create masker for PPA:\n",
    "ppa_masker = NiftiMasker(mask_img=haxby_dataset.mask_house[0], standardize=True)\n",
    "\n",
    "# Uses masker to extract numpy array for VT:\n",
    "func_ffa = ffa_masker.fit_transform(func_task)\n",
    "func_ppa = ppa_masker.fit_transform(func_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVM and outer/inner CVs:\n",
    "num_runs = len(np.unique(runs_task))\n",
    "svm = LinearSVC()\n",
    "outer_cv = KFold(n_splits=num_runs)\n",
    "inner_cv = KFold(n_splits=num_runs - 1)\n",
    "\n",
    "# Set up parameter grid:\n",
    "param_grid = {\"C\": [1e-2, 1e-1, 1]}\n",
    "\n",
    "# Initialize GridSearchCV estimator:\n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=inner_cv)\n",
    "\n",
    "# Generate predictions using cross_val_predict:\n",
    "predictions = cross_val_predict(\n",
    "    svm_cv, func_ffa, stimuli_task, cv=outer_cv, n_jobs=-1, verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy score and plot confusion matrix:\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(stimuli_task, predictions)))\n",
    "\n",
    "conf_mat = confusion_matrix(stimuli_task, predictions)\n",
    "\n",
    "\n",
    "# Plot confusion matrix:\n",
    "disp = ConfusionMatrixDisplay(conf_mat, display_labels=categories)\n",
    "disp.plot(xticks_rotation=\"vertical\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVM and outer/inner CVs:\n",
    "num_runs = len(np.unique(runs_task))\n",
    "svm = LinearSVC()\n",
    "outer_cv = KFold(n_splits=num_runs)\n",
    "inner_cv = KFold(n_splits=num_runs - 1)\n",
    "\n",
    "# Set up parameter grid:\n",
    "param_grid = {\"C\": [1e-2, 1e-1, 1]}\n",
    "\n",
    "# Initialize GridSearchCV estimator:\n",
    "svm_cv = GridSearchCV(svm, param_grid, cv=inner_cv)\n",
    "\n",
    "# Generate predictions using cross_val_predict:\n",
    "predictions = cross_val_predict(\n",
    "    svm_cv, func_ppa, stimuli_task, cv=outer_cv, n_jobs=-1, verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy score and plot confusion matrix:\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(stimuli_task, predictions)))\n",
    "\n",
    "conf_mat = confusion_matrix(stimuli_task, predictions)\n",
    "\n",
    "\n",
    "# Plot confusion matrix:\n",
    "disp = ConfusionMatrixDisplay(conf_mat, display_labels=categories)\n",
    "disp.plot(xticks_rotation=\"vertical\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> FFA does well at recognizing faces, and PPA does well at recognizing houses. Both also do a decent\n",
    "> job at distinguishing the scrambledpix from the actual objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Representational similarity analysis\n",
    "\n",
    "In this problem, we'll apply representational similarity analysis (RSA) to the human fMRI dataset from [Kriegeskorte et al., 2008](https://doi.org/10.1016/j.neuron.2008.10.043). We'll begin by loading in the ROI data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Kriegekorte dataset and labels\n",
    "kriegeskorte_dataset = dict(np.load(\"kriegeskorte_dataset.npz\", allow_pickle=True))\n",
    "\n",
    "roi_data = kriegeskorte_dataset[\"roi_data\"].item()\n",
    "category_names = kriegeskorte_dataset[\"category_names\"]\n",
    "category_labels = kriegeskorte_dataset[\"category_labels\"]\n",
    "images = kriegeskorte_dataset[\"images\"]\n",
    "subject_labels = [\"KO\", \"SN\", \"TI\"]\n",
    "roi_labels = [\"lFFA\", \"rFFA\", \"lPPA\", \"rPPA\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a `rank_percentile` function for visualizing RDMs in a way that more closely matches the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "def rank_percentile(a):\n",
    "    return rankdata(a) / len(a) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, compute RDMs for the `'lFFA'`, `'rFFA'`, `'lPPA'`, and `'rPPA'` ROIs for subject `'TI'` using correlation distance. Here, we recommend z-soring each voxel across samples prior to computing the pairwise dissimilarities. Plot the RDMs for each ROI using the `rank_percentile` function provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROI RDMs for subject TI:\n",
    "from scipy.stats import zscore\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "nrows, ncols = 2, 2\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(8, 8))\n",
    "assert nrows * ncols == len(roi_labels)\n",
    "\n",
    "subject = \"TI\"\n",
    "roi_dists = []\n",
    "for i, roi in enumerate(roi_labels):\n",
    "    ri, ci = i // ncols, i % ncols\n",
    "    ax = axes[ri, ci]\n",
    "    roi_data_ti = roi_data[subject][roi]\n",
    "    roi_data_ti = zscore(roi_data_ti, axis=0)\n",
    "    dist = pdist(roi_data_ti, metric=\"correlation\")\n",
    "    roi_dists.append(dist)\n",
    "    rdm = squareform(rank_percentile(dist))\n",
    "    im = ax.imshow(rdm, vmin=0, vmax=100, cmap=\"RdBu_r\")\n",
    "    cbar_label = \"Dissimilarity (Rank Percentile)\" if ci == 1 else None\n",
    "    fig.colorbar(im, ax=ax, fraction=0.0455, pad=0.05, label=cbar_label)\n",
    "    ax.set_title(f\"{subject} {roi} RDM\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "roi_dists = np.array(roi_dists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSA allows us to compare the representational geometries of different ROIs. Compute the correlation between each pair of the four ROIs. Plot this similarity matrix. Which ROIs have the most similar representational geometries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between each pair or ROI RDMs:\n",
    "from scipy.stats import spearmanr\n",
    "from seaborn import heatmap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "heatmap(\n",
    "    spearmanr(roi_dists, axis=1)[0],\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap=\"RdBu_r\",\n",
    "    annot=True,\n",
    "    xticklabels=roi_labels,\n",
    "    yticklabels=roi_labels,\n",
    "    ax=ax,\n",
    "    cbar_kws={\"label\": \"Spearman Correlation\"},\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The two FFA regions are most similar to each other, and same for the two PPA regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack all four ROIs to create a single combined ROI for each subject `'SN'` and `'TI'`. What is the Spearman correlation between `'SN'`'s and `'TI'`'s representational geometries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SN and TI ROIs into single VT ROI and compute RDMs:\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "ti_data = np.concatenate([roi_data[\"TI\"][roi] for roi in roi_labels], axis=1)\n",
    "ti_data = zscore(ti_data, axis=0)\n",
    "ti_dist = pdist(ti_data, metric=\"correlation\")\n",
    "sn_data = np.concatenate([roi_data[\"SN\"][roi] for roi in roi_labels], axis=1)\n",
    "sn_data = zscore(sn_data, axis=0)\n",
    "sn_dist = pdist(sn_data, metric=\"correlation\")\n",
    "\n",
    "# Compute correlations between SN and TI's VT RDMs:\n",
    "corr = spearmanr(sn_dist, ti_dist)[0]\n",
    "print(\"Spearman Correlation between SN and TI's VT RDMs: {:.3f}\".format(corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test different \"model\" RDMs according to how well they approximate a given neural. Here, for the sake of brevity, we'll construct an extremely simple RDM capturing low-level visual structure. Flatten each image file into a one-dimensional array of pixel values (across three color channels). Next, compute the pairwise Euclidean distances between these image vectors to construct an RDM capture low-level visual similarities. Plot this pixel RDM and compute it's Spearman correlation with `'TI'`s VT RDM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pixel-based RDM:\n",
    "image_data = images.reshape(images.shape[0], -1)\n",
    "image_dist = pdist(image_data, metric=\"euclidean\")\n",
    "\n",
    "# Plot RDM\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "rdm = squareform(rank_percentile(image_dist))\n",
    "im = ax.imshow(rdm, vmin=0, vmax=100, cmap=\"RdBu_r\")\n",
    "fig.colorbar(\n",
    "    im, ax=ax, fraction=0.0455, pad=0.05, label=\"Euclidean Distance (Rank Percentile)\"\n",
    ")\n",
    "ax.set_title(\"Pixel RDM\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Compute correlations with VT RDM:\n",
    "corr = spearmanr(ti_dist, image_dist)[0]\n",
    "print(\"Correlation between VT RDM and pixel-based RDM: {:.3f}\".format(corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Voxelwise encoding analysis\n",
    "\n",
    "In this problem, we'll return to *encoding analysis*, using regularized regression and out-of-sample prediction in individual voxels. We will use word embeddings derived from the natural language processing (NLP) model GloVe to map semantic encoding onto the brain. You can simply load the `story_transcript.txt` file in a text editor to visualize the transcript for the spoken story by [Carol Daniel](https://themoth.org/stories/i-knew-you-were-black). Each line of this file corresponds to a TR in the fMRI data. Next, we extracted word embeddings from GloVe for each word in each TR. For TRs containing multiple words, we averaged the embeddings. Finally, we horizontally stacked the embeddings at lags of 2, 3, 4, and 5 TRs (3, 4.5, 6, and 7.5 seconds relative to word onset) to account variable hemodynamic lags (this is effectively a finite impulse response model). Inspect and interpret the shape of the word embeddings, and visualize this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize word embeddings:\n",
    "embeddings = np.load(\"story_embeddings.npy\")\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "im = ax.imshow(embeddings, vmin=-1, vmax=1, cmap=\"RdBu_r\")\n",
    "fig.colorbar(im, ax=ax, label=\"Embedding Value (Capped)\", fraction=0.021)\n",
    "ax.set_xticks([0, 300, 600, 900, 1200])\n",
    "ax.set_ylabel(\"Time (TRs)\")\n",
    "ax.set_xlabel(\"300d GloVe Embedding (Lags of 2, 3, 4, 5 TRs)\")\n",
    "ax.set_title(\"Story Embeddings\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used fMRI to measure a subject's brain activity while they listened to the spoken story. Here, to reduce computational demands, we have spatially downsampled the fMRI data using an atlas containing 400 parcels. That is, for each parcel, we averaged the voxel time series within that parcel. Rather than fitting encoding models to tens of thousands of voxels, we'll fit our encoding model to each of the 400 parcels. Load in the `story_parcels.npy` dataset as well as the `story_atlas.nii.gz` NIfTI image from which the parcels were derived (for later visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in parcel time series:\n",
    "parcels = np.load(\"story_parcels.npy\")\n",
    "print(\"Parcels shape:\", parcels.shape)\n",
    "\n",
    "# Load in the Schaefer 400-parcel atlas:\n",
    "import nibabel as nib\n",
    "\n",
    "atlas = nib.load(\"story_atlas.nii.gz\")\n",
    "print(\"Atlas shape:\", atlas.shape)\n",
    "\n",
    "atlas_labels = np.unique(atlas.get_fdata())\n",
    "assert parcels.shape[1] == len(atlas_labels) - 1  # -1 for background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our word embedding \"model\" is much wider than the number of samples, so we'll need to use regularization and out-of-sample prediction to mitigate overfitting. We'll use ridge regression to fit encoding models to predict the parcel time series from the word embeddings. First, set up an split-half outer cross-validator using `KFold` with `n_splits=2`; next, set up an inner cross-validator using `KFold` with `n_splits=5` to perform grid search for the `alpha` hyperparameter using 5-fold cross-validation within each training set of the otuer loop. Initialize your `RidgeCV` estimator with the inner cross-validator and the following grid of alphas: `alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]`. For each training and testing split of the other cross-validation loop, fit the ridge model on the training set of embeddings and parcel time series, and generate predicted parcel time series from the test embeddings. Compile these predicted parcel time series for model evaluation in the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up outer/inner cross-validators:\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "outer_cv = KFold(n_splits=2)\n",
    "inner_cv = KFold(n_splits=5)\n",
    "\n",
    "\n",
    "# Initialize RidgeCV with alpha grid and inner CV:\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-1, 4, 6)\n",
    "ridge = RidgeCV(alphas=alphas, cv=inner_cv)\n",
    "\n",
    "\n",
    "# Loop through outer CV loop, fit model, generate predictions:\n",
    "predictions = cross_val_predict(ridge, embeddings, parcels, cv=outer_cv, n_jobs=-1)\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our encoding model's predictions, correlate the predicted parcel time series with the actual parcel time series for each parcel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation between predicted and actual responses:\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "corrs = np.zeros(parcels.shape[1])\n",
    "for i in range(parcels.shape[1]):\n",
    "    corrs[i] = pearsonr(parcels[:, i], predictions[:, i]).statistic\n",
    "\n",
    "full_corr = pearsonr(parcels.ravel(), predictions.ravel()).statistic\n",
    "print(\"Overall correlation: {:.3f}\".format(full_corr))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.hist(corrs, bins=25)\n",
    "ax.axvline(full_corr, color=\"red\", linestyle=\"dashed\", linewidth=2)\n",
    "ax.set_xlabel(\"Predicted vs. Actual Correlation\")\n",
    "ax.set_ylabel(\"Number of Parcels\")\n",
    "ax.set_title(\"Encoding Model Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to visualize the performance of our semantic encoding model on the brain, we need to use the atlast NIfTI image to convert from parcels back to the original brain image. You can start by creating an empty brain image (i.e. zeros) the size of the atlas image. Next, loop through each parcel and insert the prediction scores (i.e. correlations between actual and predicted parcel time series) into all voxels where the atlas correponds to that parcel label. Convert this image to a NIfTI image and visualize with `plot_stat_map`; you may want to set a particular `vmax` and use a `threshold` to exclude voxels with poor prediction performance for the sake of visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty brain image and populate with parcelwise performance values:\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "brain_image = np.zeros(atlas.shape)\n",
    "atlas_data = atlas.get_fdata()\n",
    "for i, corr in enumerate(corrs):\n",
    "    label = i + 1\n",
    "    brain_image[atlas_data == label] = corr\n",
    "\n",
    "# Convert to NIfTI image for visualization with Nilearn:\n",
    "from nibabel import Nifti1Image\n",
    "\n",
    "brain_image = Nifti1Image(brain_image, atlas.affine, atlas.header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations to visualize superior temporal cortex:\n",
    "plot_stat_map(\n",
    "    brain_image,\n",
    "    cut_coords=(-55, -24, 9),\n",
    "    title=\"Encoding Model Performance in STC\",\n",
    "    colorbar=True,\n",
    "    threshold=0.05,\n",
    "    vmax=0.35,\n",
    "    cmap=\"RdBu_r\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations to visualize posterior medial cortex:\n",
    "plot_stat_map(\n",
    "    brain_image,\n",
    "    cut_coords=(-5, -60, 30),\n",
    "    title=\"Encoding Model Performance in PMC\",\n",
    "    colorbar=True,\n",
    "    threshold=0.05,\n",
    "    vmax=0.35,\n",
    "    cmap=\"RdBu_r\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
