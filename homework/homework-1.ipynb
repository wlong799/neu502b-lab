{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU502B: Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following homework assignment will require you to (1) load and visualize (f)MRI data in several different ways, (2) visualize several common confound variables, (3) create a design matrix capturing the experimental condition, and (4) use regression to model fMRI activity. Each of these problems builds on tools and ideas we've introduced in the in-class lab notebooks. We'll start by loading in some general-purpose Python modules, but you'll need to load additional modules to complete the problems (look to the lab notebooks for examples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.glm.first_level import glover_hrf\n",
    "from nilearn.image import mean_img, index_img\n",
    "from pandas import read_table\n",
    "from scipy.special import eval_legendre\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Visualization\n",
    "\n",
    "fMRI datasets are complex and noisy, so it's important to visualize your data at every stage of analysis. We'll begin by loading in the dataset from [Haxby et al., 2001](https://doi.org/10.1126/science.1063736) using [Nilearn](https://nilearn.github.io/). You'll need to change `data_dir` to a directory on your computer (or the server); if you've already downloaded this dataset in lab, you can set `data_dir` to the existing directory to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haxby et al., 2001 data via Nilearn\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=\"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first set of exercises, we'll visualize (*a*) the T1-weighted anatomical image, (*b*) the EPI image averaged across time, (*c*) an EPI volume at time point 1312, and (*d*) a mask demarcating ventral temporal (VT) cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the T1-weighted anatomical image here:\n",
    "plotting.plot_anat(haxby_dataset.anat[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mean EPI image here:\n",
    "mean_epi = mean_img(haxby_dataset.func[0])\n",
    "plotting.plot_epi(mean_epi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EPI volume (time point) 1312 here:\n",
    "idx_image = index_img(haxby_dataset.func[0], 1312)\n",
    "plotting.plot_epi(idx_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the VT mask overlaid on the mean EPI here:\n",
    "plotting.plot_roi(haxby_dataset.mask_vt[0], mean_epi, cmap=\"Paired\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use [NiBabel](https://nipy.org/nibabel/) to directly load in the data as [NumPy](https://numpy.org/) arrays for manipulation in Python. First, we'll load in the VT mask as a boolean array. Inspect the shape of the functional data and mask, apply the mask to the functional data to get an array containing EPI time series for only VT cortex. Inspect the shape of the masked EPI data. Next, plot the mean time series in VT. Finally, without using Nilearn, create a \"carpet plot\" via [Power et al., 2017](https://doi.org/10.1016/j.neuroimage.2016.08.009) for VT data where the x-axis corresponds to time and the y-axis corresponds to voxels. Make sure to z-score each voxel's time series prior to plotting (e.g. using `zscore` from `scipy.stats`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NiBabel to load functional data and VT mask:\n",
    "anat_img = nib.load(haxby_dataset.anat[0]).get_fdata()\n",
    "func_img = nib.load(haxby_dataset.func[0]).get_fdata()\n",
    "mask_img = nib.load(haxby_dataset.mask_vt[0]).get_fdata()\n",
    "print(anat_img.shape, func_img.shape, mask_img.shape)\n",
    "\n",
    "# Keep the VT mask voxel indices for later:\n",
    "mask_indices = np.where(mask_img)\n",
    "print(mask_indices[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask functional data and inspect shapes:\n",
    "masked_func_data = func_img[mask_indices]\n",
    "\n",
    "# You may want to transpose the masked data so that time points are in zeroth dimension:\n",
    "masked_func_data = masked_func_data.T\n",
    "print(masked_func_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean time series in VT cortex here:\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(masked_func_data.mean(axis=1))\n",
    "ax.set_xlabel(\"TR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot carpet plot of VT data here:\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(zscore(masked_func_data, axis=0).T, cmap=\"gray\")\n",
    "ax.set_xlabel(\"TR\")\n",
    "ax.set_ylabel(\"Voxel\")\n",
    "ax.set_title(\"VT Carpet Plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Confounds\n",
    "Unfortunately the publicly-available [Haxby et al., 2001](https://doi.org/10.1126/science.1063736) dataset does not include confound variables. For this exercise, we'll take a short detour to visualize some confounds from the sample data accompanying the [Princeton Handbook for Reproducible Neuroimaging](https://brainhack-princeton.github.io/handbook/). These confounds variables are created by fMRIPrep during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds_f = \"sub-001_ses-01_task-faces_run-1_desc-confounds_timeseries.tsv\"\n",
    "df = read_table(confounds_f, sep=\"\\t\")\n",
    "n_trs = len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, extract the six head motion parameters comprising translation (_x_-, _y_-, _z_-axes) and rotation (roll, pitch, yaw) from the counfounds table. Plot the translation and rotation time series below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract head motion parameters and visualize:\n",
    "translation = df[[\"trans_x\", \"trans_y\", \"trans_z\"]]\n",
    "rotation = df[[\"rot_x\", \"rot_y\", \"rot_z\"]]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "axs[0].plot(translation)\n",
    "axs[0].set_ylabel(\"Translation\")\n",
    "axs[0].legend([\"X\", \"Y\", \"Z\"])\n",
    "axs[1].plot(rotation)\n",
    "axs[1].set_ylabel(\"Rotation\")\n",
    "axs[1].legend([\"Roll\", \"Pitch\", \"Yaw\"])\n",
    "axs[1].set_xlabel(\"TR\")\n",
    "fig.suptitle(\"Head Motion Parameters\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll obtain the aCompCor confounds returned by fMRIPrep. These correspond to principal component (PC) time series extracted from anatomically-defined masks cerebrospinal fluid (CSF) and white matter, which may reflect physiological fluctuations and other noise sources. Extract the first 5 aCompCor time series and plot them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 5 anatomical CompCor signals and plot:\n",
    "acompcors = df[[f\"a_comp_cor_0{i}\" for i in range(5)]]\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(acompcors)\n",
    "ax.set_xlabel(\"TR\")\n",
    "ax.legend([\"aCompCor 0\", \"aCompCor 1\", \"aCompCor 2\", \"aCompCor 3\", \"aCompCor 4\"])\n",
    "ax.set_title(\"Anatomical Confounds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fMRI data often contain slow, non-neural fluctuations over time due to thermal noise and other measurement artifacts. One way to mitigate these slow noise fluctuations is to construct detrending variables. Here, we use 4th-degree [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials) to construct a set of variables that will account for slow drifts in the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 4th-degree Legendre polynomials\n",
    "x_grid = np.linspace(-1, 1, n_trs)\n",
    "degree = 4\n",
    "\n",
    "polys = []\n",
    "for n in range(degree + 1):\n",
    "    polys.append(eval_legendre(n, x_grid))\n",
    "polys = np.column_stack(polys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the polynomial detrending variables below on a `tr_grid` corresponding to the number of TRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot polynomial detrending variables on TR grid:\n",
    "tr_grid = np.arange(n_trs)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(tr_grid, polys)\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend([\"0\", \"1\", \"2\", \"3\", \"4\"])\n",
    "ax.set_title(\"Legendre Polynomials\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll compile all of these time series into a confound matrix. Column-stack the polynomial trends, head motion, and aCompCor variables into a single confound matrix and plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confound matrix:\n",
    "confounds = np.column_stack([polys, translation, rotation, acompcors])\n",
    "fig, ax = plt.subplots(figsize=(5, 10))\n",
    "im = ax.imshow(confounds, aspect=\"auto\")\n",
    "ax.set_xlabel(\"Confounds\")\n",
    "ax.set_ylabel(\"Time\")\n",
    "ax.set_xticks([])\n",
    "fig.suptitle(\"Confound Matrix\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Design\n",
    "\n",
    "In fMRI analysis, we usually assume that the BOLD signal is the result of a linear time-invariant (LTI) system ([Boynton et al., 1996](http://www.jneurosci.org/content/16/13/4207)); in other words, we assume (1) the shape of the hemodynamic response is constant across time, and (2) the responses to successive stimuli superpose linearly (additively). Assumption #2 justifies using convolution in generating the predicted BOLD signal from a set of stimulus events, $x$, and a hemodynamic response function, $h$. Conceptually, what convolution does is add the entire HRF shape starting wherever there is a non-zero entry.\n",
    "\n",
    "In the [Haxby et al., 2001](https://doi.org/10.1126/science.1063736) experiment, participants were presented with images from 8 object categories (faces, cats, shoes, scissors, bottles, chairs, houses, and scrambled images) interspersed with periods of fixation (referred to as \"rest\" here). The TR in this study was 2.5 seconds. In a given run, a block of images from each of the 8 categories was presented one time. Each block was ~9 TRs long and contained multiple rapid presentations of images from a single category. A subject received 12 scanning runs. In the public dataset, these runs are concatenated into a single time series 1452 TRs in duration (although we will split it back into individual runs for certain processing steps). Below, we load in the stimulus and run labels and set some parameters for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in session metadata as pandas DataFrame\n",
    "session = read_table(haxby_dataset.session_target[0], sep=\" \")\n",
    "\n",
    "# Extract stimuli and run labels for this subject\n",
    "stimuli, runs = session[\"labels\"].values, session[\"chunks\"].values\n",
    "\n",
    "# Stimulus categories\n",
    "categories = [\n",
    "    \"face\",\n",
    "    \"cat\",\n",
    "    \"shoe\",\n",
    "    \"scissors\",\n",
    "    \"bottle\",\n",
    "    \"chair\",\n",
    "    \"house\",\n",
    "    \"scrambledpix\",\n",
    "]\n",
    "n_categories = len(categories)\n",
    "\n",
    "# Set some basic experimental parameters\n",
    "tr = 2.5\n",
    "n_trs = 1452\n",
    "n_runs = 12\n",
    "run_trs = 121\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll construct one boxcar time series for each of the 8 stimulus categories in the first run. Note that in this experiment each run only contains one block of stimuli per category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct boxcar time series for the first run\n",
    "run_id = 0\n",
    "boxcars = []\n",
    "for category in categories:\n",
    "    boxcar = np.zeros(run_trs)\n",
    "    boxcar[stimuli[runs == run_id] == category] = 1\n",
    "    boxcars.append(boxcar)\n",
    "boxcars = np.column_stack(boxcars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we plot the boxcar time series for each of the 8 stimulus categories. Note that we do not explicitly model the 'rest' fixation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxcar time series for the first run\n",
    "fig, axs = plt.subplots(n_categories, 1, figsize=(8, 4), sharex=True, sharey=True)\n",
    "for i, (category, boxcar) in enumerate(zip(categories, boxcars.T)):\n",
    "    axs[i].plot(boxcar)\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_ylabel(category, rotation=0, ha=\"right\", va=\"center\")\n",
    "axs[-1].set_xlabel(\"TR\")\n",
    "fig.suptitle(\"Boxcar Time Series (Run 1)\")\n",
    "sns.despine()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load in a canonical hemodynamic response function (HRF) sampled at our 2.5-second TR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 2.5\n",
    "hrf = glover_hrf(tr, oversampling=1, time_length=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current boxcar time series capture hypothesized neural activity in response to the stimulus images, but do not reflect the sluggish BOLD response. For now, we'll keep focusing on the first run. In the following cell, convolve each of these boxcar time series with the canonical HRF to construct regressors for the fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve first-run boxcar time series with HRF:\n",
    "convolved = np.zeros((run_trs, n_categories))\n",
    "for i in range(n_categories):\n",
    "    convolved[:, i] = np.convolve(boxcars[:, i], hrf)[:run_trs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the predicted BOLD time series similarly to the boxcar time series above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted BOLD time series for first run:\n",
    "fig, axs = plt.subplots(n_categories, 1, figsize=(8, 4), sharex=True, sharey=True)\n",
    "for i, (category, convolved_boxcar) in enumerate(zip(categories, convolved.T)):\n",
    "    axs[i].plot(convolved_boxcar)\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_ylabel(category, rotation=0, ha=\"right\", va=\"center\")\n",
    "axs[-1].set_xlabel(\"TR\")\n",
    "axs[-1].set_xlim(0, convolved.shape[0])\n",
    "fig.suptitle(\"Predicted BOLD Time Series (Run 1)\")\n",
    "sns.despine()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, combining elements of the code from previous cells, repeat this procedure of convolving each within-run boxcar time series with the HRF for all 12 runs. Vertical-stack all of these runwise regressors into a single design matrix `X` with shape `(1452, 8)`. Plot the eight regressors across the entire experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through runs, apply HRF, and re-stack:\n",
    "convolved_all = np.zeros((n_trs, n_categories))\n",
    "for run in range(n_runs):\n",
    "    boxcars = []\n",
    "    for category in categories:\n",
    "        boxcar = np.zeros(run_trs)\n",
    "        boxcar[stimuli[runs == run] == category] = 1\n",
    "        boxcars.append(boxcar)\n",
    "    boxcars = np.column_stack(boxcars)\n",
    "    for i in range(n_categories):\n",
    "        conv = np.convolve(boxcars[:, i], hrf)[:run_trs]\n",
    "        convolved_all[run * run_trs : (run + 1) * run_trs, i] = conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regressors across entire experiment:\n",
    "fig, axs = plt.subplots(n_categories, 1, figsize=(8, 4), sharex=True, sharey=True)\n",
    "for i, (category, convolved_boxcar) in enumerate(zip(categories, convolved_all.T)):\n",
    "    axs[i].plot(convolved_boxcar)\n",
    "    axs[i].set_yticks([])\n",
    "    axs[i].set_ylabel(category, rotation=0, ha=\"right\", va=\"center\")\n",
    "axs[-1].set_xlabel(\"TR\")\n",
    "axs[-1].set_xlim(0, n_trs)\n",
    "fig.suptitle(\"Predicted BOLD Time Series (All Runs)\")\n",
    "sns.despine()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing an fMRI experiment, it's important minimize collinearity among your regressors of interest to ensure that your regression model can accurately assign variance to each regressor. One simple way to evaluate collinearity is to compute the pairwise correlations (shaped `(8, 8)`) among your regressors of interest in `X`. Use `np.corrcoef` to compute the pairwise correlations among regressors and plot the resulting correlation matrix; similarly, you could compute the convariance matrix using `np.cov` or `X.T @ X`, or use a more sophisticated metric like [variance inflation factor](https://en.wikipedia.org/wiki/Variance_inflation_factor) (e.g. [`variance_inflation_factor`](https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html) in `statsmodels.stats.outliers_influence`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix and plot:\n",
    "corr = np.corrcoef(convolved_all.T)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask, k=1)] = 1\n",
    "min_corr = corr[mask.astype(bool)].min()\n",
    "max_corr = corr[mask.astype(bool)].max()\n",
    "im = ax.imshow(corr, cmap=\"RdBu_r\", alpha=mask, vmin=min_corr, vmax=max_corr)\n",
    "ax.set_xticks(np.arange(n_categories))\n",
    "ax.set_xticklabels(categories, rotation=90)\n",
    "ax.set_yticks(np.arange(n_categories))\n",
    "ax.set_yticklabels(categories)\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Regression\n",
    "\n",
    "Now that we have a set of regressors (i.e. predictors) capturing the object categories in our experimental design, we'll use a simple regression analysis to model each voxel time series in the fMRI data. In fMRI analysis, this is often referred to as the mass univariate general linear model or GLM. The goal is to discover which voxels are most responsive to certain object categories such as faces or houses. Recall that the publicly-available [Haxby et al., 2001](https://doi.org/10.1126/science.1063736) unfortunately does not include confound variables, so here we'll focus mostly on the regressors of interest. However, we can include 4th-order Legendre polynomials in the same way as above to mitigate any slow drifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 4th-degree Legendre polynomials matching number of TRs:\n",
    "degree = 4\n",
    "xs = np.linspace(-1, 1, n_trs)\n",
    "legendres = []\n",
    "for i in range(degree + 1):\n",
    "    legendres.append(eval_legendre(i, xs))\n",
    "legendres = np.column_stack(legendres)\n",
    "print(legendres.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack these polynomial detrending variables alongside the eight category regressors in your design matrix `X`. Keep track of which columns correspond to variables of interest and which correspond to confound variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack detrending variables alongside category regressors:\n",
    "X = np.column_stack([convolved_all, legendres])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to make sure that our fMRI data are prepared. For the sake of computational efficiency, we'll focus on the voxels in VT cortex, rather than running a whole-brain analysis across all voxels. Use the functional data extracted from VT cortex from Part 1. Split the VT data into separate runs, z-score the voxelwise time series within each run, the re-stack the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score each run and re-stack:\n",
    "y = np.zeros_like(masked_func_data)\n",
    "for run in range(n_runs):\n",
    "    idx_slice = slice(run * run_trs, (run + 1) * run_trs)\n",
    "    y[idx_slice] = zscore(masked_func_data[idx_slice], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should be ready to run our regression. First, to keep things as simple as possible, we'll run the regression using `lstsq` from `np.linalg`. Use `lstsq` to compute the regression coefficients (i.e. \"beta values\") for the eight category regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use least-squares to compute betas:\n",
    "betas = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "print(betas.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use a contrast vector to perform two comparisons: (1) faces > all seven other categories, and (2) houses > all seven other categories. Remember, these contrast vectors should sum to zero. Multiple the contrast vector by the corresponding betas and sum across betas to compute the contrast; each comparison should result in a single map for VT cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrast vectors:\n",
    "face_contrast = np.zeros(betas.shape[0])\n",
    "face_contrast[:n_categories] = -1\n",
    "face_contrast[categories.index(\"face\")] = n_categories - 1\n",
    "\n",
    "house_contrast = np.zeros(betas.shape[0])\n",
    "house_contrast[:n_categories] = -1\n",
    "house_contrast[categories.index(\"house\")] = n_categories - 1\n",
    "\n",
    "# Apply contrast vectors to betas:\n",
    "face_betas = betas.T @ face_contrast\n",
    "house_betas = betas.T @ house_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, reinsert these beta values back into a full NIfTI image and use `plot_stat_map` from `nilearn.plotting` to visualize the resulting maps for the faces-vs-all contrast and the houses-vs-all contrast. Try using the T1-weighted anatomical image as the `bg_img`, then pick a good set of coordinates for visualizing VT (e.g. `cut_coords = (-43, -40, -12)`). What do you notice about the anatomical localization of face-responsive versus house-responsive areas of VT cortex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "func_affine = nib.load(haxby_dataset.func[0]).affine\n",
    "\n",
    "# Reinsert VT face contrast map into full NIfTI image:\n",
    "face_img = np.zeros_like(mask_img)\n",
    "face_img[mask_indices] = face_betas\n",
    "print(face_img.shape)\n",
    "face_nifti = nib.Nifti1Image(face_img, affine=func_affine)\n",
    "\n",
    "# Plot face-vs-all contrast:\n",
    "plot_stat_map(\n",
    "    face_nifti,\n",
    "    bg_img=haxby_dataset.anat[0],\n",
    "    title=\"Face vs. All\",\n",
    "    cut_coords=(-43, -40, -12),\n",
    "    cmap=\"RdBu_r\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinsert VT house contrast map into full NIfTI image:\n",
    "house_img = np.zeros_like(mask_img)\n",
    "house_img[mask_indices] = house_betas\n",
    "house_nifti = nib.Nifti1Image(house_img, affine=func_affine)\n",
    "\n",
    "# Plot house-vs-all contrast:\n",
    "plot_stat_map(\n",
    "    house_nifti,\n",
    "    bg_img=haxby_dataset.anat[0],\n",
    "    title=\"House vs. All\",\n",
    "    cut_coords=(-43, -40, -12),\n",
    "    cmap=\"RdBu_r\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "09eede0490220c6e4a7a286cfdee92f8bcd25bd1c59649ec16c2b8768d1e527a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
