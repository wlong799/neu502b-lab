{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `elec-01`: ERP analysis\n",
    "This lab introduces epoching and event-related potential (ERP) analysis using MNE. We will apply the\n",
    "ERP analysis to EEG responses to visual and auditory stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example data and events\n",
    "We'll load in our preprocessed EEG data acquired while participants were presented with auditory and\n",
    "visual stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.io import read_raw_fif\n",
    "\n",
    "# Load preprocessed data\n",
    "raw_fn = \"../elec-00/sub-01_task-audvis_preproc_raw.fif\"\n",
    "raw = read_raw_fif(raw_fn, preload=True, verbose=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the EEG and peripheral channels, our recording includes trigger (i.e. stimulus)\n",
    "channels. Trigger channels mark the onset/offset of events during recording. In our recording in\n",
    "particular, STI 014 is the trigger channel that was used for combining all the events to a single\n",
    "channel. It has several pulses of different amplitude throughout the recording. These pulses\n",
    "correspond to different stimuli presented to the subject during the acquisition. The pulses and\n",
    "their corresponding events are defined in the table below.\n",
    "\n",
    "| Name   | #  | Contents                                |\n",
    "|--------|----|-----------------------------------------|\n",
    "| LA     | 1  | Response to left-ear auditory stimulus  |\n",
    "| RA     | 2  | Response to right-ear auditory stimulus |\n",
    "| LV     | 3  | Response to left visual field stimulus  |\n",
    "| RV     | 4  | Response to right visual field stimulus |\n",
    "| Smiley | 5  | Response to the smiley face             |\n",
    "| Button | 32 | Response triggered by the button press  |\n",
    "\n",
    "These are the events to which we'll align our epochs. To create an event list from raw data, we'll\n",
    "use a dedicated function. Since the event list is simply a numpy array, you can also manually create\n",
    "one. If you create one from an outside source (like a separate file of events), pay special\n",
    "attention to aligning the events correctly with the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import find_events\n",
    "from mne.viz import plot_events\n",
    "\n",
    "# Extract events\n",
    "events = find_events(raw)\n",
    "print(events[:5])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event list contains three columns. The first column corresponds to sample number. To convert\n",
    "this to seconds, you must divide the sample number by the sampling frequency. The second column is\n",
    "reserved for the old value of the trigger channel at the time of transition, but is currently not in\n",
    "use. The third column is the trigger ID (amplitude of the pulse). To get a better picture of the\n",
    "task design, we'll plot out the events. First we'll construct a Python dictionary matching event\n",
    "labels to event integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot events\n",
    "event_id = {\n",
    "    \"left auditory\": 1,\n",
    "    \"right auditory\": 2,\n",
    "    \"left visual\": 3,\n",
    "    \"right visual\": 4,\n",
    "    \"smiley\": 5,\n",
    "    \"button\": 32,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n",
    "color = {\n",
    "    1: \"#1f77b4\",\n",
    "    2: \"#ff7f0e\",\n",
    "    3: \"#2ca02c\",\n",
    "    4: \"#d62728\",\n",
    "    5: \"#9467bd\",\n",
    "    32: \"#8c564b\",\n",
    "}\n",
    "fig = plot_events(\n",
    "    events, raw.info[\"sfreq\"], raw.first_samp, color=color, event_id=event_id, axes=ax\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoching\n",
    "\n",
    "Epoching describes the process of taking snapshots (i.e. windows) of the data centered around some\n",
    "event of interest. We will perform epoching using the MNE `Epochs` class. To do so, we need to\n",
    "define some parameters for our epoching. In this tutorial we are only interested in triggers 1, 2, 3\n",
    "and 4. These triggers correspond to auditory and visual stimuli. The `event_id` here can be an\n",
    "integer, a list of integers or a dictionary. With dictionaries, it's possible to assign these IDs to\n",
    "distinct categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine events of interest\n",
    "event_id = {\n",
    "    \"left auditory\": 1,\n",
    "    \"right auditory\": 2,\n",
    "    \"left visual\": 3,\n",
    "    \"right visual\": 4,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the windows of interest. The values `tmin` and `tmax` refer to offsets in\n",
    "relation to the events. Here we make epochs that collect the data from -200 ms before to 500 ms\n",
    "after the event. To get some meaningful results, we also want to baseline the epochs. Baselining\n",
    "computes the mean over the baseline period and adjusts the data accordingly. The `Epochs` class uses\n",
    "a baseline period from `tmin` to 0.0 seconds by default, but we will set it explicitly to avoid any\n",
    "surprises. Including `None` as the first element of the tuple refers to the start of the time window\n",
    "(-200 ms in this case). See [`mne.Epochs`](https://mne.tools/stable/generated/mne.Epochs.html) for\n",
    "more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epoch lengths\n",
    "tmin = -0.2\n",
    "tmax = 0.5\n",
    "baseline = (None, -0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our rejection parameters for peak-to-peak amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rejection threshold\n",
    "reject = {\"eeg\": 100e-6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we perform epoching, choosing only the EEG channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import Epochs, pick_types\n",
    "\n",
    "# Perform epoching\n",
    "picks = pick_types(raw.info, meg=False, eeg=True)\n",
    "epochs = Epochs(\n",
    "    raw,\n",
    "    events,\n",
    "    event_id=event_id,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    baseline=baseline,\n",
    "    picks=picks,\n",
    "    reject=reject,\n",
    "    preload=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we crop the window and remove all bad epochs (i.e. those violating amplitude rejection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop epochs\n",
    "epochs = epochs.crop(tmin=-0.1)\n",
    "\n",
    "# Drop bad epochs\n",
    "epochs.drop_bad()\n",
    "print(epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the data; note the naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting epochs file\n",
    "fout = \"sub-01_task-audvis-epo.fif\"\n",
    "epochs.save(fout, overwrite=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERP visualization\n",
    "\n",
    "Now that we have defined our epochs, we can inspect the event-related potentials (ERPs). There are a\n",
    "great many example tutorials on visualizing evoked potentials\n",
    "[here](https://mne.tools/stable/auto_tutorials/evoked/plot_20_visualize_evoked.html). We demonstrate\n",
    "a few below. First, we compute the evoked response for each visual stimulus condition and plot the\n",
    "ERPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average within each condition\n",
    "LV_evoked = epochs[\"left visual\"].average()\n",
    "RV_evoked = epochs[\"right visual\"].average()\n",
    "\n",
    "# Plot ERPs\n",
    "print(\"left visual\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "fig = LV_evoked.plot(spatial_colors=True, xlim=(-0.1, 0.5), axes=ax)\n",
    "\n",
    "print(\"right visual\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "fig = RV_evoked.plot(spatial_colors=True, xlim=(-0.1, 0.5), axes=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the scalp topographic maps for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scalp topographies\n",
    "print(\"left visual\")\n",
    "fig = LV_evoked.plot_topomap(times=np.arange(0.05, 0.25, 0.025))\n",
    "print(\"right visual\")\n",
    "fig = RV_evoked.plot_topomap(times=np.arange(0.05, 0.25, 0.025))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sensor layout from the previous notebook, we can choose a sensor that is clearly picking up a response to the visual stimuli. Here we are observing strong laterality, so we visualize two sensors: `EEG 056` and `EEG 057`. We will clearly see that that right- and left-presented stimuli are more strongly represented in the contralateral hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import permutation_cluster_test\n",
    "\n",
    "# Extract data\n",
    "eeg_056 = epochs.copy().pick_channels([\"EEG 056\"])\n",
    "data = [\n",
    "    eeg_056[\"left visual\"].get_data().squeeze(),\n",
    "    eeg_056[\"right visual\"].get_data().squeeze(),\n",
    "]\n",
    "evokeds = [np.mean(data[0], axis=0) * 1e6, np.mean(data[1], axis=0) * 1e6]\n",
    "\n",
    "# Using F-statistic as default\n",
    "# F-stat = abs(t-stat ** 2)\n",
    "F_obs, clusters, cluster_pv, H0 = permutation_cluster_test(\n",
    "    data, n_permutations=1024, seed=47404, tail=1\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "ax.plot(epochs.times, evokeds[0], lw=2.5, label=\"left visual\")  # Cond: LV\n",
    "ax.plot(epochs.times, evokeds[1], lw=2.5, label=\"right visual\")  # Cond: RV\n",
    "# ax.plot(epochs.times, F_obs, lw=2, color='0.2', label='F-vals') # F-stats\n",
    "ymin, ymax = ax.get_ylim()\n",
    "\n",
    "# Plot clusters\n",
    "for cluster, pval in zip(clusters, cluster_pv):\n",
    "    if pval < 0.05:\n",
    "        center = epochs.times[cluster].mean()\n",
    "        ax.fill_between(epochs.times[cluster], ymin, ymax, color=\"0.8\", alpha=0.5)\n",
    "        ax.annotate(\n",
    "            \"p = %0.3f\" % pval,\n",
    "            (0, 0),\n",
    "            (center, ymax),\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "\n",
    "# Add details\n",
    "ax.hlines(0, epochs.tmin, epochs.tmax, linewidth=0.5, alpha=0.5, zorder=0)\n",
    "ax.set(\n",
    "    xlim=(epochs.tmin, epochs.tmax),\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylim=(ymin, ymax),\n",
    "    ylabel=\"uV\",\n",
    "    title=\"EEG 056 (right visual cortex)\",\n",
    ")\n",
    "ax.legend(loc=4, frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import permutation_cluster_test\n",
    "\n",
    "# Extract data\n",
    "eeg_057 = epochs.copy().pick_channels([\"EEG 057\"])\n",
    "data = [\n",
    "    eeg_057[\"left visual\"].get_data().squeeze(),\n",
    "    eeg_057[\"right visual\"].get_data().squeeze(),\n",
    "]\n",
    "evokeds = [np.mean(data[0], axis=0) * 1e6, np.mean(data[1], axis=0) * 1e6]\n",
    "\n",
    "# Using F-statistic as default\n",
    "# F-stat = abs(t-stat ** 2)\n",
    "F_obs, clusters, cluster_pv, H0 = permutation_cluster_test(\n",
    "    data, n_permutations=1024, seed=47404, tail=1\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "ax.plot(epochs.times, evokeds[0], lw=2.5, label=\"left visual\")  # Cond: LV\n",
    "ax.plot(epochs.times, evokeds[1], lw=2.5, label=\"right visual\")  # Cond: RV\n",
    "# ax.plot(epochs.times, F_obs, lw=2, color='0.2', label='F-vals') # F-stats\n",
    "ymin, ymax = ax.get_ylim()\n",
    "\n",
    "# Plot clusters\n",
    "for cluster, pval in zip(clusters, cluster_pv):\n",
    "    if pval < 0.05:\n",
    "        center = epochs.times[cluster].mean()\n",
    "        ax.fill_between(epochs.times[cluster], ymin, ymax, color=\"0.8\", alpha=0.5)\n",
    "        ax.annotate(\n",
    "            \"p = %0.3f\" % pval,\n",
    "            (0, 0),\n",
    "            (center, ymax),\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "\n",
    "# Add details\n",
    "ax.hlines(0, epochs.tmin, epochs.tmax, linewidth=0.5, alpha=0.5, zorder=0)\n",
    "ax.set(\n",
    "    xlim=(epochs.tmin, epochs.tmax),\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylim=(ymin, ymax),\n",
    "    ylabel=\"uV\",\n",
    "    title=\"EEG 057 (left visual cortex)\",\n",
    ")\n",
    "ax.legend(loc=4, frameon=False)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the difference between the two ERPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import combine_evoked\n",
    "\n",
    "# Compute difference wave\n",
    "DV_evoked = combine_evoked([LV_evoked, RV_evoked], [-1, 1])\n",
    "\n",
    "# Plot difference waves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "for ch in [\"EEG 056\", \"EEG 057\"]:\n",
    "    ax.plot(\n",
    "        DV_evoked.times,\n",
    "        DV_evoked.data[DV_evoked.ch_names.index(ch)] * 1e6,\n",
    "        lw=2,\n",
    "        label=ch,\n",
    "    )\n",
    "\n",
    "ax.set(xlabel=\"time (s)\", ylabel=\"uV\", title=\"difference (right - left)\")\n",
    "ax.legend(loc=1, frameon=False)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
